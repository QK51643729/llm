# 学习笔记

## 模型量化

> [权重+激活量化是一种深度学习模型压缩方法，通过将权重和激活值转换为较低精度表示，减少模型大小和计算复杂度。这种方法在移动端推理、边缘计算、嵌入式系统等领域具有广泛应用，可以显著提高推理速度、降低存储空间需求和减小计算复杂度。具体实现方法包括二值化量化、8位量化、自适应量化等。](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247642336&idx=1&sn=87746fb00404df4d5545159f7b9e523d&chksm=e9efa56bde982c7d8b1db9b2634c91637470cd479ad8213d497a22d2a4d794aabe97ab3c111e&scene=27)

上述是一种PTQ（Post-Training Quantization）方法，在训练完成后进行压缩而无需再次训练。
## 模型剪枝


## 知识蒸馏
[知识蒸馏综述](https://cloud.tencent.com/developer/article/1814300)
![logits蒸馏权重](image.png)
- Logit-based KD 是一种基于输出概率的知识蒸馏方法，它通过最小化学生模型和教师模型之间的输出概率差异来实现知识传递。(控温技术:先高后低)
- Feature-based KD 是一种基于特征的知识蒸馏方法，它通过匹配教师模型和学生模型的输出特征来传递知识。这种方法要求学生模型不仅要知道结果，还要理解底层过程。
- Relation-based KD 是一种基于关系的知识蒸馏方法，它旨在让学生模型学习教师模型如何处理关系型知识。这种关系主要体现在两个方面：同一样本在不同层的输出关系和不同样本的输出关系。
- Black-box KD是一种黑盒知识蒸馏方法，它不需要访问教师模型的内部信息，而是通过教师模型的预测结果来传递知识。这种方法适用于大型模型的知识蒸馏，因为大型模型的内部信息通常是不可访问的。

### 高效注意力
Transformer中的标准自注意力机制时空复杂度为O(N2)，阻碍了处理长序列问题的能力。为了解决这个问题，出现了高效注意力工作，包括稀疏注意力、线性近似注意力和闪存注意力等。
- 稀疏注意力方法可以分为基于全局、基于窗口和基于数据三种方法。基于全局的方法包括全局注意力、基于窗口的方法包括局部注意力和固定注意力模式，基于数据的方法包括随机注意力和数据驱动的稀疏注意力。
- 线性近似注意力计算方法可以分为基于关联性和低秩的方法。
